{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import statsmodels.api as sm\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tennis Surface Check\n",
    "\n",
    "Use a linear regression and statsmodels to find which surface type predicts the most points for Federer in the `tennis.csv` dataset.\n",
    "\n",
    "1. Give a one-paragraph interpretation of the coefficients, and the meaning of the p-value. \n",
    "\n",
    "2. Answer the following: should your regression include a constant term? Why or why not? How would it change the interpretation of your coefficient and p-value?\n",
    "\n",
    "3. Do a t-test to find that the largest coefficient is statistically significantly different from the second largest (hint: you can run a t-test only with mean values and standard deviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               OLS Regression Results                               \n",
      "====================================================================================\n",
      "Dep. Variable:     player1 total points won   R-squared:                       0.048\n",
      "Model:                                  OLS   Adj. R-squared:                  0.044\n",
      "Method:                       Least Squares   F-statistic:                     16.93\n",
      "Date:                      Sat, 30 Jan 2021   Prob (F-statistic):           1.91e-13\n",
      "Time:                              01:12:26   Log-Likelihood:                -4793.2\n",
      "No. Observations:                      1016   AIC:                             9596.\n",
      "Df Residuals:                          1011   BIC:                             9621.\n",
      "Df Model:                                 4                                         \n",
      "Covariance Type:                        HC2                                         \n",
      "==================================================================================\n",
      "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "const             83.2281      3.411     24.403      0.000      76.543      89.913\n",
      "Indoor: Hard      -6.1456      3.629     -1.693      0.090     -13.259       0.968\n",
      "Outdoor: Clay      4.8599      3.990      1.218      0.223      -2.961      12.680\n",
      "Outdoor: Grass    15.4655      4.524      3.419      0.001       6.599      24.332\n",
      "Outdoor: Hard      4.5908      3.659      1.255      0.210      -2.581      11.762\n",
      "==============================================================================\n",
      "Omnibus:                      141.907   Durbin-Watson:                   1.575\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              232.211\n",
      "Skew:                           0.915   Prob(JB):                     3.77e-51\n",
      "Kurtosis:                       4.463   Cond. No.                         11.0\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors are heteroscedasticity robust (HC2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-115.45380964705573, pvalue=0.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('data/tennis.csv')\n",
    "df = df.dropna()\n",
    "\n",
    "print(sm.OLS(df['player1 total points won'], sm.add_constant(pd.get_dummies(df.surface, drop_first=True))).fit(cov_type='HC2').summary())\n",
    "\n",
    "\n",
    "#1.1 after adding the constanct when the coef is high means that he is perform better Outdoor: Grass and Outdoor: Clay with the highest coef \n",
    "#for the P valu eis high 85% accurate \n",
    "\n",
    "#1.2 we should not add the constant because it will affect r-quared and the p value\n",
    "\n",
    "#3\n",
    "sp.stats.ttest_ind_from_stats(-6.1456,4.090, 1016, 15.4655, 4.344, 1016)\n",
    "#It is statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Titanic prediction contest\n",
    "\n",
    "Use whatever tricks you can to best model whether a passenger would survive the titanic disaster (using linear regression).\n",
    "\n",
    "1. Use non-regularized regression to build the best model you can. Show 2 alternate model speficications and explain why you chose the one you did\n",
    "\n",
    "2. Interpret the coefficients in your model. Which attributes best relate to survival probability? How does this relate to socio-economic characteristics and \"real-world\" interpretation?\n",
    "\n",
    "3. Use regularized regression to build a purely predictive model. Can you improve your accuracy? Plot the regularized model against the interpretable model predictions in a regression plot to make your case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#For the model to work, we first need to clean this dataset.\n",
    "df = sn.load_dataset('titanic')\n",
    "df = df.sort_values(by='pclass')\n",
    "\n",
    "# Convert datatypes to something more workable.\n",
    "df.embarked = df.embarked.astype(str)\n",
    "df['age'] = df.age.astype(float)\n",
    "\n",
    "df['sex'] = df.sex.replace(('male', 'female'), (1, 0))\n",
    "df['adult_male'] = df['adult_male'].replace((True, False), (1, 0))\n",
    "df['alive'] = df['alive'].replace((True, False), (1, 0))\n",
    "df['alone'] = df['alone'].replace((True, False), (1, 0))\n",
    "\n",
    "whom = pd.get_dummies(df['who'], drop_first=True)\n",
    "embark = pd.get_dummies(df['embarked'], drop_first=True)\n",
    "\n",
    "#Drop the duplicate columns. Deck would be a useful variable but we just don't have enough data to make it useful. I will also make a copy to save for alternative models.\n",
    "df_ = df.copy()\n",
    "df = df.drop(['embark_town', 'class', 'deck'], axis=1)\n",
    "df = df.dropna()\n",
    "\n",
    "## 1.\n",
    "# y = df.survived\n",
    "# x = df[['pclass', 'fare', 'sex', 'sibsp', 'parch', 'adult_male', 'alone']]\n",
    "# X = sm.add_constant(x)\n",
    "# X = X.join(whom)\n",
    "# X = X.join(embark)\n",
    "# sm.OLS(y,X).fit(cov_type='HC2').summary()\n",
    "\n",
    "## After a review of the p-values, I can remove some less-than-useful variables. For the sake of interpretability I dropped 'parch' due to its weak coefficient.\n",
    "\n",
    "y = df.survived\n",
    "x = df[['pclass', 'sibsp', 'adult_male', 'alone']]\n",
    "\n",
    "X = sm.add_constant(x)\n",
    "my_model = sm.OLS(y,X).fit(cov_type='HC2')\n",
    "# my_model.summary()\n",
    "## Alright, so the R-squared value is not fantastic but the p-values and coefficient make this a useable model, so it is the model I will go with. Lets try a few others.\n",
    "\n",
    "## Alternative model #1.\n",
    "## I really want to know the effect of deck on survival rate. Because of the small sample, we will need to drastically reduce the size of the dataset to do so.\n",
    "\n",
    "df_ = df_.drop(['embark_town', 'class'], axis=1)\n",
    "df_ = df_.dropna()\n",
    "\n",
    "y = df_.survived\n",
    "x = df_[['pclass', 'sibsp', 'adult_male', 'alone']]\n",
    "X = sm.add_constant(x)\n",
    "X = X.join(pd.get_dummies(df_['deck'], drop_first=True))\n",
    "\n",
    "alt_model1 = sm.OLS(y,X).fit(cov_type='HC2')\n",
    "# alt_model1.summary()\n",
    "\n",
    "#Okay, I tried playing around with this but its just bad. Statistically insignificant (p-value) and the R-squared is next to nothing.\n",
    "\n",
    "## Alternative model #2. \n",
    "## I was picky in the first model, so for this one we will 'kitchen-sink' style add every variable that has a significant p-value.\n",
    "\n",
    "y = df.survived\n",
    "x = df[['pclass', 'sibsp', 'parch', 'adult_male', 'alone']]\n",
    "X = sm.add_constant(x)\n",
    "alt_model2 = sm.OLS(y,X).fit(cov_type='HC2')\n",
    "# alt_model2.summary()\n",
    "\n",
    "#Yea, its slightly better. I don't think the increased predictability merits the extra coefficients.\n",
    "\n",
    "## 2.\n",
    "## The two strongest coefficient are 'adult_male', showing that adult_males were more likely not to survive, and 'pclass', showing that people in upper classes were more likely to survive than lower classes. The remaining 'sibsp' and 'alone' both demonstrate improved survival rates for those with family on board. \n",
    "\n",
    "## What we learn from this is that there is at least some truth to the idea that they saved women and children first when the Titanic sank. From a sociopolitical perspective, it is unsurprising that the upper classes with family to rely on were more likely to survive than the lower classes and those who were alone.\n",
    "\n",
    "##3.\n",
    "y = df.survived\n",
    "x = df[['pclass', 'sibsp', 'adult_male', 'alone']]\n",
    "\n",
    "X = sm.add_constant(x)\n",
    "my_model = sm.OLS(y,X).fit(cov_type='HC2')\n",
    "reg_model = sm.OLS(y,X).fit_regularized(method='elastic_net', alpha=0.2)\n",
    "\n",
    "yfit = my_model.predict(X)\n",
    "regfit = reg_model.predict(X)\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X['pclass'], y, color='purple', alpha=0.2)\n",
    "ax.plot(X['pclass'], regfit, color='red', alpha=0.8, linewidth=3)\n",
    "ax.plot(X['pclass'], yfit, color='blue', alpha=0.8, linewidth=3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Price prediction\n",
    "\n",
    "Using the techniques you learned, use everything you can to build the best **interpretable** (eg. non-regularized) regression model on the `house_price.csv` dataset. You also have `house_price_data_description.txt` to help -- full description of each column.\n",
    "\n",
    "Here's a brief version of what you'll find in the data description file.\n",
    "\n",
    "**SalePrice** - the property's sale price in dollars. **This is the target variable that you're trying to predict.**\n",
    "\n",
    "Here are the features you can use (or engineer into new features!) for your `X` matrix:\n",
    "\n",
    "    MSSubClass: The building class\n",
    "    MSZoning: The general zoning classification\n",
    "    LotFrontage: Linear feet of street connected to property\n",
    "    LotArea: Lot size in square feet\n",
    "    Street: Type of road access\n",
    "    Alley: Type of alley access\n",
    "    LotShape: General shape of property\n",
    "    LandContour: Flatness of the property\n",
    "    Utilities: Type of utilities available\n",
    "    LotConfig: Lot configuration\n",
    "    LandSlope: Slope of property\n",
    "    Neighborhood: Physical locations within Ames city limits\n",
    "    Condition1: Proximity to main road or railroad\n",
    "    Condition2: Proximity to main road or railroad (if a second is present)\n",
    "    BldgType: Type of dwelling\n",
    "    HouseStyle: Style of dwelling\n",
    "    OverallQual: Overall material and finish quality\n",
    "    OverallCond: Overall condition rating\n",
    "    YearBuilt: Original construction date\n",
    "    YearRemodAdd: Remodel date\n",
    "    RoofStyle: Type of roof\n",
    "    RoofMatl: Roof material\n",
    "    Exterior1st: Exterior covering on house\n",
    "    Exterior2nd: Exterior covering on house (if more than one material)\n",
    "    MasVnrType: Masonry veneer type\n",
    "    MasVnrArea: Masonry veneer area in square feet\n",
    "    ExterQual: Exterior material quality\n",
    "    ExterCond: Present condition of the material on the exterior\n",
    "    Foundation: Type of foundation\n",
    "    BsmtQual: Height of the basement\n",
    "    BsmtCond: General condition of the basement\n",
    "    BsmtExposure: Walkout or garden level basement walls\n",
    "    BsmtFinType1: Quality of basement finished area\n",
    "    BsmtFinSF1: Type 1 finished square feet\n",
    "    BsmtFinType2: Quality of second finished area (if present)\n",
    "    BsmtFinSF2: Type 2 finished square feet\n",
    "    BsmtUnfSF: Unfinished square feet of basement area\n",
    "    TotalBsmtSF: Total square feet of basement area\n",
    "    Heating: Type of heating\n",
    "    HeatingQC: Heating quality and condition\n",
    "    CentralAir: Central air conditioning\n",
    "    Electrical: Electrical system\n",
    "    1stFlrSF: First Floor square feet\n",
    "    2ndFlrSF: Second floor square feet\n",
    "    LowQualFinSF: Low quality finished square feet (all floors)\n",
    "    GrLivArea: Above grade (ground) living area square feet\n",
    "    BsmtFullBath: Basement full bathrooms\n",
    "    BsmtHalfBath: Basement half bathrooms\n",
    "    FullBath: Full bathrooms above grade\n",
    "    HalfBath: Half baths above grade\n",
    "    Bedroom: Number of bedrooms above basement level\n",
    "    Kitchen: Number of kitchens\n",
    "    KitchenQual: Kitchen quality\n",
    "    TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n",
    "    Functional: Home functionality rating\n",
    "    Fireplaces: Number of fireplaces\n",
    "    FireplaceQu: Fireplace quality\n",
    "    GarageType: Garage location\n",
    "    GarageYrBlt: Year garage was built\n",
    "    GarageFinish: Interior finish of the garage\n",
    "    GarageCars: Size of garage in car capacity\n",
    "    GarageArea: Size of garage in square feet\n",
    "    GarageQual: Garage quality\n",
    "    GarageCond: Garage condition\n",
    "    PavedDrive: Paved driveway\n",
    "    WoodDeckSF: Wood deck area in square feet\n",
    "    OpenPorchSF: Open porch area in square feet\n",
    "    EnclosedPorch: Enclosed porch area in square feet\n",
    "    3SsnPorch: Three season porch area in square feet\n",
    "    ScreenPorch: Screen porch area in square feet\n",
    "    PoolArea: Pool area in square feet\n",
    "    PoolQC: Pool quality\n",
    "    Fence: Fence quality\n",
    "    MiscFeature: Miscellaneous feature not covered in other categories\n",
    "    MiscVal: $Value of miscellaneous feature\n",
    "    MoSold: Month Sold\n",
    "    YrSold: Year Sold\n",
    "    SaleType: Type of sale\n",
    "    SaleCondition: Condition of sale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data\\house_price.csv')\n",
    "\n",
    "#This dataframe is huge and intimidating. I'm going to make a function that returns the most relevant variables.\n",
    "\n",
    "# df.dtypes.unique() #find the datatypes\n",
    "\n",
    "def good_vars(df, corr_degree):\n",
    "    final_list = []\n",
    "    objects = []\n",
    "    object_frame = df[['SalePrice']]\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == np.object:\n",
    "            objects.append(col)\n",
    "            object_frame = object_frame.join( pd.get_dummies(df[col], prefix=col, drop_first=True) )\n",
    "\n",
    "    df_ = df.drop(objects, axis=1)\n",
    "\n",
    "    for var in df_.columns:\n",
    "        if df_.astype(float).corr()['SalePrice'][var] > corr_degree:\n",
    "            final_list.append(var)\n",
    "        else:\n",
    "            pass\n",
    "    for var in object_frame:\n",
    "        if object_frame.astype(float).corr()['SalePrice'][var] > corr_degree:\n",
    "            final_list.append(var)\n",
    "        else:\n",
    "            pass   \n",
    "\n",
    "    return final_list\n",
    "\n",
    "var_list = good_vars(df, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>SalePrice</td>    <th>  R-squared:         </th> <td>   0.759</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.759</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   415.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 30 Jan 2021</td> <th>  Prob (F-statistic):</th> <td>5.65e-277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>01:15:13</td>     <th>  Log-Likelihood:    </th> <td> -17504.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  1460</td>      <th>  AIC:               </th> <td>3.502e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  1454</td>      <th>  BIC:               </th> <td>3.505e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>         <td>HC2</td>       <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>        <td>-1.343e+06</td> <td>  9.9e+04</td> <td>  -13.572</td> <td> 0.000</td> <td>-1.54e+06</td> <td>-1.15e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>OverallQual</th>  <td>  2.26e+04</td> <td> 1678.212</td> <td>   13.465</td> <td> 0.000</td> <td> 1.93e+04</td> <td> 2.59e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>YearBuilt</th>    <td>  371.2535</td> <td>   43.318</td> <td>    8.570</td> <td> 0.000</td> <td>  286.351</td> <td>  456.156</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>YearRemodAdd</th> <td>  270.6144</td> <td>   51.073</td> <td>    5.299</td> <td> 0.000</td> <td>  170.513</td> <td>  370.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>1stFlrSF</th>     <td>   36.3111</td> <td>    7.720</td> <td>    4.703</td> <td> 0.000</td> <td>   21.180</td> <td>   51.442</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GrLivArea</th>    <td>   49.4390</td> <td>    6.320</td> <td>    7.823</td> <td> 0.000</td> <td>   37.052</td> <td>   61.826</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>404.189</td> <th>  Durbin-Watson:     </th> <td>   1.994</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>35289.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.118</td>  <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>27.084</td>  <th>  Cond. No.          </th> <td>4.03e+05</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors are heteroscedasticity robust (HC2)<br/>[2] The condition number is large, 4.03e+05. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              SalePrice   R-squared:                       0.759\n",
       "Model:                            OLS   Adj. R-squared:                  0.759\n",
       "Method:                 Least Squares   F-statistic:                     415.3\n",
       "Date:                Sat, 30 Jan 2021   Prob (F-statistic):          5.65e-277\n",
       "Time:                        01:15:13   Log-Likelihood:                -17504.\n",
       "No. Observations:                1460   AIC:                         3.502e+04\n",
       "Df Residuals:                    1454   BIC:                         3.505e+04\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:                  HC2                                         \n",
       "================================================================================\n",
       "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "const        -1.343e+06    9.9e+04    -13.572      0.000   -1.54e+06   -1.15e+06\n",
       "OverallQual    2.26e+04   1678.212     13.465      0.000    1.93e+04    2.59e+04\n",
       "YearBuilt      371.2535     43.318      8.570      0.000     286.351     456.156\n",
       "YearRemodAdd   270.6144     51.073      5.299      0.000     170.513     370.716\n",
       "1stFlrSF        36.3111      7.720      4.703      0.000      21.180      51.442\n",
       "GrLivArea       49.4390      6.320      7.823      0.000      37.052      61.826\n",
       "==============================================================================\n",
       "Omnibus:                      404.189   Durbin-Watson:                   1.994\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            35289.431\n",
       "Skew:                          -0.118   Prob(JB):                         0.00\n",
       "Kurtosis:                      27.084   Cond. No.                     4.03e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors are heteroscedasticity robust (HC2)\n",
       "[2] The condition number is large, 4.03e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Alright, let's take that list and run with it.\n",
    "y = df.SalePrice\n",
    "x = df[var_list]\n",
    "X = sm.add_constant(x)\n",
    "\n",
    "model = sm.OLS(y,X).fit(cov_type='HC2')\n",
    "\n",
    "# model.summary()\n",
    "#Alright, this isn't perfect. Let's remove the high p-values\n",
    "\n",
    "y = df.SalePrice\n",
    "x = df[['OverallQual', 'YearBuilt', 'YearRemodAdd', '1stFlrSF', 'GrLivArea']]\n",
    "X = sm.add_constant(x)\n",
    "\n",
    "model = sm.OLS(y,X).fit(cov_type='HC2')\n",
    "model.summary()\n",
    "\n",
    "#Not a bad model if I do say so myself. Low p-values, pretty good R-squared and strong dollar values in relation to the coeefficients.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
